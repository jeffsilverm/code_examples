{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeff's wordnet idea\n",
    "A wordnet is a data structure similar to a thesarus\n",
    "A wordnet is a collection of synsets.  Each synset is a collection of words with a similar meaning.\n",
    "\n",
    "The structure of this page roughly follows the wordnet page in the NLTK documentation, http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print ( sys.version, file=sys.stderr )\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pandas as pd\n",
    "import preprocessing\n",
    "from typing import TypeVar\n",
    "import nltk\n",
    "\n",
    "def make_synset_from_word_list(component_words) -> nltk.corpus.reader.wordnet.Synset :\n",
    "    \"\"\"\n",
    "    This subroutine returns a synset which is the union of the all of the synsets of\n",
    "    all of the words in the component (which is either a 'body' or a 'title'.\n",
    "    \n",
    "    param: component_words    A list of words in the title or the body of this joke\n",
    "    \"\"\"\n",
    "    \n",
    "    component_synset = set({})\n",
    "    for word in component_words:\n",
    "        word_synset = wn.synset(word)\n",
    "        assert isinstance(word_synset[0], nltk.corpus.reader.wordnet.Synset)\n",
    "        component_synset |= word_synset\n",
    "    return component_synset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n",
      "The type of the values of dictionary 'title' is  <class 'pandas.core.series.Series'>\n",
      "The type of the values of dictionary 'body' is  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading the jokes JSON file\", file=sys.stderr)\n",
    "jokeData = pd.read_json(\"reddit_jokes.json\")\n",
    "print(\"Preprocessing\", file=sys.stderr)\n",
    "prep = preprocessing.preprocess()\n",
    "print(\"Storing the jokeData dictionary\", file=sys.stderr)\n",
    "jokeData['body']: pd.core.series.Series = prep.cleanData(jokeData['body'])\n",
    "jokeData['title']: pd.core.series.Series = prep.cleanData(jokeData['title'])\n",
    "print(\"The type of the values of dictionary 'title' is \", type(jokeData['title']))\n",
    "print(\"The type of the values of dictionary 'body' is \", type(jokeData['body']))\n",
    "\n",
    "print(\"Tokenizing the words\", file=sys.stderr)\n",
    "# title_word_list is a list of all of the tokenized words in each title.  If there are 194394 jokes in the JSON\n",
    "# file, then this list will be of length 194394\n",
    "title_word_list = prep.tokenizeText(jokeData['title'])\n",
    "body_word_list = prep.tokenizeText(jokeData['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(type(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "194394\n",
      "['whats', 'difference', 'jew', 'nazi', 'germany', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "print(type(words))\n",
    "print(len(words))\n",
    "print(words[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for title_words, body_words in zip(title_word_list, body_word_list :\n",
    "    body_synset = make_synset_from_word_list(body_words)\n",
    "    title_synset = make_synset_from_word_list(title_words)\n",
    "    similarity_list = list()\n",
    "    for title_syn_elem in title_synset:\n",
    "        for body_syn_elem in body_synset:\n",
    "            distance = wn.path_similarity(title_syn_elem, body_syn_elem)\n",
    "            similarity_list.append( ( distance, title_syn_elem, body_syn_elem))\n",
    "    similarity_list.sort()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'Synset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-36908e7639cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msyns\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Type of syns is %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'Synset'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syns =wn.synsets('dog') \n",
    "assert isinstance(syns, wn.Synset),\"Type of syns is %s\" % type(syns)\n",
    "print(type(syns[0]))\n",
    "s:set =set()\n",
    "print(\"There are %d items in set s (should be 0)\" % len(s))\n",
    "s.add(\"Q\", \"R\")\n",
    "print(\"Set s contains %s, should be Q, R \" % str(s) )\n",
    "s ^= {\"W\", \"T\", \"R\"}\n",
    "print(\"Set s contains %s, should be Q, R, T, W\" % str(s) )\n",
    "s |= {\"W\", \"p\"}\n",
    "print(\"Set s contains %s, should be Q R, T, W, p \"% str(s) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
