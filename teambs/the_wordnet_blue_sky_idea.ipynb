{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeff's wordnet idea\n",
    "A wordnet is a data structure similar to a thesarus\n",
    "A wordnet is a collection of synsets.  Each synset is a collection of words with a similar meaning.\n",
    "\n",
    "The structure of this page roughly follows the wordnet page in the NLTK documentation, http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Oct 13 2017, 12:02:49) \n[GCC 7.2.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whats', 'difference', 'jew', 'nazi', 'germany', 'pizza']\n                                                body      id  score  \\\n0                       say leroy please paint fence  5tz52q      1   \n1              pizza doesnt scream put oven im sorry  5tz4dd      0   \n2  really helped learn american culture visited s...  5tz319      0   \n3  sunday school teacher concerned students might...  5tz2wj      1   \n4          got caught trying sell two books freshman  5tz1pc      0   \n\n                                               title  \n0             hate cant even say black paint anymore  \n1            whats difference jew nazi germany pizza  \n2                              recently went america  \n3                  brian raises hand says hes heaven  \n4  hear university book store worker charged stea...  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print ( sys.version, file=sys.stderr )\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pandas as pd\n",
    "import preprocessing\n",
    "from typing import TypeVar\n",
    "import nltk\n",
    "\n",
    "def make_synset_from_word_list(component_words) -> nltk.corpus.reader.wordnet.Synset :\n",
    "    \"\"\"\n",
    "    This subroutine returns a synset which is the union of the all of the synsets of\n",
    "    all of the words in the component (which is either a 'body' or a 'title'.\n",
    "    \n",
    "    param: component_words    A list of words in the title or the body of this joke\n",
    "    \"\"\"\n",
    "    \n",
    "    component_synset = set({})\n",
    "    for word in component_words:\n",
    "        word_synset = wn.synset(word)\n",
    "        assert isinstance(word_synset[0], nltk.corpus.reader.wordnet.Synset)\n",
    "        component_synset |= word_synset\n",
    "    return component_synset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n",
      "The type of the values of dictionary 'title' is  <class 'pandas.core.series.Series'>\n",
      "The type of the values of dictionary 'body' is  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading the jokes JSON file\", file=sys.stderr)\n",
    "jokeData = pd.read_json(\"reddit_jokes.json\")\n",
    "print(\"Preprocessing\", file=sys.stderr)\n",
    "prep = preprocessing.preprocess()\n",
    "print(\"Storing the jokeData dictionary\", file=sys.stderr)\n",
    "jokeData['body']: pd.core.series.Series = prep.cleanData(jokeData['body'])\n",
    "jokeData['title']: pd.core.series.Series = prep.cleanData(jokeData['title'])\n",
    "print(\"The type of the values of dictionary 'title' is \", type(jokeData['title']))\n",
    "print(\"The type of the values of dictionary 'body' is \", type(jokeData['body']))\n",
    "\n",
    "print(\"Tokenizing the words\", file=sys.stderr)\n",
    "# title_word_list is a list of all of the tokenized words in each title.  If there are 194394 jokes in the JSON\n",
    "# file, then this list will be of length 194394\n",
    "title_word_list = prep.tokenizeText(jokeData['title'])\n",
    "body_word_list = prep.tokenizeText(jokeData['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(type(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "194394\n",
      "['whats', 'difference', 'jew', 'nazi', 'germany', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "print(type(words))\n",
    "print(len(words))\n",
    "print(words[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for title_words, body_words in zip(title_word_list, body_word_list) :\n",
    "    body_synset = make_synset_from_word_list(body_words)\n",
    "    title_synset = make_synset_from_word_list(title_words)\n",
    "    similarity_list = list()\n",
    "    for title_syn_elem in title_synset:\n",
    "        for body_syn_elem in body_synset:\n",
    "            distance = wn.path_similarity(title_syn_elem, body_syn_elem)\n",
    "            similarity_list.append( ( distance, title_syn_elem, body_syn_elem))\n",
    "    similarity_list.sort()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.wordnet.Synset'>\nThere are 0 items in set s (should be 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.wordnet.Synset'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'set'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7018be445c93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"There are %d items in set s (should be 0)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"R\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Set s contains %s, should be Q, R \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m^=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"R\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'set'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import sys\n",
    "syns =wn.synsets('dog')\n",
    "print(type(syns[0]), file=sys.stderr)\n",
    "assert isinstance(syns[0], nltk.corpus.reader.wordnet.Synset),\"Type of syns is %s\" % \\\n",
    "                                                    type(syns)\n",
    "print(type(syns[0]))\n",
    "s:set =set()\n",
    "print(\"There are %d items in set s (should be 0)\" % len(s))\n",
    "s |= {\"Q\", \"R\"}\n",
    "print(\"Set s contains %s, should be Q, R \" % str(s) )\n",
    "# Update the set, keeping only elements found in either set, but not in both.\n",
    "s ^= {\"W\", \"T\", \"R\"}\n",
    "print(\"Set s contains %s, should be Q, T, W\" % str(s) )\n",
    "# Intersection\n",
    "s &= {\"W\", \"p\"}\n",
    "print(\"Set s contains %s, should be W \"% str(s) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
